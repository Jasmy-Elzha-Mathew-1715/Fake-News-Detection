{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scrapping",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIzXRoaYlViB2eqkD0QRDv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEsSJL_MxDVf"
      },
      "source": [
        "!pip install nest_asyncio\n",
        "\n",
        "!pip install twint\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyd2r8PKxG_T"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #PASTE CODE GENERATED BY GOING TO GIVEN URL AND SELECTING YOUR GOOGLE DRIVE ACCOUNT WHERE DATASETS ARE UPLOADED"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69gm-JY5xOxi"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#UPLOAD THE DATASETS TO YOUR DRIVE FIRST \n",
        "path = \"/content/drive/My Drive/Internship/fakenews.csv\" #CHANGE THE PATH AND NAME ACCORDING TO YOUR DRIVE AND DATASET USED RESPECTIVELY\n",
        "fakenews = pd.read_csv(path)\n",
        "path = \"/content/drive/My Drive/Internship/realnews.csv\" #CHANGE THE PATH AND NAME ACCORDING TO YOUR DRIVE AND DATASET USED RESPECTIVELY\n",
        "realnews = pd.read_csv(path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcHxNhguxUPX"
      },
      "source": [
        "import nest_asyncio\n",
        "import twint\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "columns = [\"headline\"]\n",
        "faketweets = pd.DataFrame(columns=columns) #TO STORE THE SCRAPPED FAKE NEWS TWEETS\n",
        "\n",
        "n = len(fakenews.index)\n",
        "\n",
        "faketweets=faketweets.drop(faketweets.index[:]) #ONLY USE WHEN YOU WANT TO CLEAR ALL ROWS FROM THE DATASET\n",
        "\n",
        "for i in range(10000): #LOOP USED TO COLLECT TWEETS FOR FIRST n HEADLINES(YOU CAN CHANGE THE RANGE TO GET TWEETS AS PER YOUR REQUIREMENTS)\n",
        "  #configuration\n",
        "  config = twint.Config()\n",
        "  config.Search = fakenews.iloc[i,0] #TAKE iTH HEADLINE FROM THE FAKE NEWS DATASET \n",
        "  config.Limit = 20 #GETS A MAXIMUM OF 20 TWEETS\n",
        "  config.Pandas = True\n",
        "  config.Stats = True\n",
        "  config.Count = True\n",
        "  config.Filter_retweets = True #SET TO TRUE IF YOU WANT TO REMOVE RETWEETS\n",
        "  config.Min_likes = 1\n",
        "  config.Min_retweets = 1\n",
        " \n",
        "  #running search\n",
        "  twint.run.Search(config)\n",
        "  Tweets = twint.storage.panda.Tweets_df #STORES THE SCRAPPED FAKE NEWS TWEETS\n",
        "  print(\"Headline:\",i)\n",
        "  faketweets = faketweets.append(Tweets,ignore_index = True) #ADDS FAKE NEWS TWEETS TO THE FAKETWEETS DATASET\n",
        "  \n",
        " realtweets.to_csv('location.csv') #Store as csv in desired location"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}