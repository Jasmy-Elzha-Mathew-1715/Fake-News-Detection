{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Visualisation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZu5WKkgAcMF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeWj7XmltXCT"
      },
      "source": [
        "fake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1Snjux6ti4G"
      },
      "source": [
        "real = pd.read_csv('/content/drive/My Drive/Internship/Scrapped Data/realtweetsfinal.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WweaJoWfttsT"
      },
      "source": [
        "real"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV-HpSIatxE4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "plt.figure(figsize = (10, 7)) \n",
        "x = fake[\"nlikes\"] \n",
        "y = real[\"nlikes\"]\n",
        "  \n",
        "plt.hist(x, bins = 20, color = \"green\") \n",
        "plt.hist(y, bins = 20, color = \"red\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1laz-R-nulCS"
      },
      "source": [
        "new_data = fake[[\"nlikes\",\"nreplies\",\"nretweets\"]]\n",
        "plt.figure(figsize = (10, 7)) \n",
        "new_data.boxplot() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUKXeDHNv8bv"
      },
      "source": [
        "sns.distplot(fake['nretweets'], bins=10, kde=False)\n",
        "sns.distplot(real['nretweets'], bins=10, kde=False)\n",
        "fake = fake[\"nretweets\"]\n",
        "real = real[\"nretweets\"]\n",
        "plt.hist([\n",
        "         fake,\n",
        "         real\n",
        "    ],\n",
        "     stacked=True,\n",
        "     label=[\"fake\", \"real\"])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_9USUXWBs_N"
      },
      "source": [
        "def get_candidate(row):\n",
        "    candidates = []\n",
        "    text = row[\"tweet\"].lower()\n",
        "    if \"clinton\" in text or \"hillary\" in text:\n",
        "        candidates.append(\"clinton\")\n",
        "    if \"trump\" in text or \"donald\" in text:\n",
        "        candidates.append(\"trump\")\n",
        "    if \"sanders\" in text or \"bernie\" in text:\n",
        "        candidates.append(\"sanders\")\n",
        "    return \",\".join(candidates)\n",
        "tweets[\"candidate\"] = tweets.apply(get_candidate,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isebytyx7wra"
      },
      "source": [
        "def get_candidate(row):\n",
        "    candidates = [\"AddInfoOrg\",\"CraigCo62\",\"StayHopeful16\",\"buffaloon\",\"coopah\"]\n",
        "    text = row[\"tweet\"].lower()\n",
        "tweet[\"candidate\"] = tweet.apply(get_candidate,axis=1)\n",
        "\n",
        "counts = tweet[\"candidate\"].value_counts()\n",
        "plt.bar(range(len(counts)), counts)\n",
        "plt.show()\n",
        "print(counts)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XRThBu0C2v7"
      },
      "source": [
        "import pandas.\n",
        "from pandas.tools.plotting import andrews_curves\n",
        "andrews_curves(fake.drop(\"username\", axis=1), \"tweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUc3Dv8FCJVY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUgN2r6mCPCZ"
      },
      "source": [
        "counts = tweets[\"candidate\"].value_counts()\n",
        "plt.bar(range(len(counts)), counts)\n",
        "plt.show()\n",
        "print(counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj99fK3vYNOu"
      },
      "source": [
        "fake[\"username\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbTdnqqcm-Kp"
      },
      "source": [
        "fake[\"tweet\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF9c19R-n7It"
      },
      "source": [
        "fake_sample = fake[\"tweet\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDfug_hxoClV"
      },
      "source": [
        "fake_sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpME5wGAoNVn"
      },
      "source": [
        "topfaketweet = fake_sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjwAyv__omz5"
      },
      "source": [
        "new_data = topfaketweet[[\"tweet\",\"count\"]]\n",
        "plt.figure(figsize = (10, 7)) \n",
        "new_data.boxplot() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssCl0bifnjxj"
      },
      "source": [
        "real[\"tweet\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nVXOxsblQ5h"
      },
      "source": [
        "real[\"username\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ReanqleYZxR"
      },
      "source": [
        "fake.plot(kind=\"scatter\", x=\"nlikes\", y=\"nretweets\")\n",
        "plt.xlabel(\"Likes\")\n",
        "plt.ylabel(\"Retweets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RG_5SsgHl6V"
      },
      "source": [
        "real.plot(kind=\"scatter\", x=\"nlikes\", y=\"nretweets\")\n",
        "plt.xlabel(\"Likes\")\n",
        "plt.ylabel(\"Retweets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toEs4oRs13l0"
      },
      "source": [
        "fake.plot(kind=\"scatter\", x=\"nreplies\", y=\"nretweets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDCIQ20o2DN2"
      },
      "source": [
        "fake.plot(kind=\"scatter\", x=\"nlikes\", y=\"nreplies\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIVcSymBY2fv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "sns.FacetGrid(fake, hue=\"tweet\", size=5) \\\n",
        "   .map(plt.scatter, \"nlikes\", \"nretweets\") \\\n",
        "   .add_legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaiYsrUfiy_Z"
      },
      "source": [
        "sns.pairplot(fake.drop(\"tweet\", axis=1), hue=\"nlikes\", size=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPHpYccWzx3V"
      },
      "source": [
        "d = fake['nlikes']\n",
        "f = fake['nreplies']\n",
        "plt.scatter(\"d\", \"f\", color='r')\n",
        "plt.xlabel('likes')\n",
        "plt.ylabel('retweets')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6A3daMOJ_CM"
      },
      "source": [
        "fake_df = pd.read_csv('/content/drive/My Drive/Internship/Scrapped Data/faketweetsfinal.csv')\n",
        "true_df = pd.read_csv('/content/drive/My Drive/Internship/Scrapped Data/realtweetsfinal.csv')\n",
        "\n",
        "fake_df['fake'] = 1\n",
        "true_df['fake'] = 0\n",
        "\n",
        "data_df = pd.concat([fake_df, true_df], ignore_index=True)\n",
        "data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab5-hu45KkJw"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "ax = sns.countplot(x=\"fake\", data=data_df)\n",
        "ax.set_xticklabels(['Real', 'Fake'])\n",
        "plt.xlabel('Classification')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmr7abjoMCRV"
      },
      "source": [
        "data_df['search_length'] = data_df['search'].apply(lambda x : len(x.strip().split()))\n",
        "data_df['tweet_length'] = data_df['tweet'].apply(lambda x : len(x.strip().split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kew8A6eeMjvD"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.distplot(data_df[data_df['fake'] == 1]['search_length'], \n",
        "             kde=False, label='Fake', bins=20)\n",
        "sns.distplot(data_df[data_df['fake'] == 0]['search_length'], \n",
        "             kde=False, label='True', bins=20)\n",
        "plt.xlabel('Search Length', weight='bold')\n",
        "plt.title('Length of search comparison', weight='bold')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-ScRnfxNhit"
      },
      "source": [
        "fig = plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Word counts of search\", fontsize=16, weight='bold')\n",
        "ax = sns.boxplot(x=\"fake\", y=\"search_length\", data=data_df)\n",
        "ax.set_xticklabels(['Real', 'Fake'])\n",
        "ax.set_xlabel(\"Search Classification\", fontsize=14, weight='bold') \n",
        "ax.set_ylabel(\"Length of Entry (Words)\", fontsize=14, weight='bold')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjLE4PXbPZVW"
      },
      "source": [
        "def create_corpus(text_data):\n",
        "    \"\"\" Create a corpus from the given text array of sentences \"\"\"\n",
        "    corpus = []\n",
        "    for sentence in text_data:\n",
        "        for word in sentence.split():\n",
        "            corpus.append(word)\n",
        "    return corpus\n",
        "            \n",
        "def top_words(text_corpus, top_n=25, return_dict=False):\n",
        "    \"\"\" Return the top n words from a given corpus \"\"\"\n",
        "    def_dict = defaultdict(int)\n",
        "    for word in text_corpus:\n",
        "        def_dict[word] += 1\n",
        "    most_common = sorted(def_dict.items(), key=lambda x : x[1], reverse=True)[:top_n]\n",
        "    if return_dict:\n",
        "        return most_common, def_dict\n",
        "    else:    \n",
        "        return most_common"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYl5gp9WQGQF"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "pd.plotting.register_matplotlib_converters()\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "import re\n",
        "import umap\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "from matplotlib.colors import ListedColormap\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "\n",
        "from sklearn.decomposition import PCA, LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, \\\n",
        "                            classification_report, f1_score, roc_curve, auc\n",
        "from sklearn.model_selection import cross_validate, cross_val_score, train_test_split, StratifiedKFold\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# import classical ml models\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOetYL2oPiKG"
      },
      "source": [
        "top_n = 50\n",
        "text_field = \"search\"\n",
        "\n",
        "fake_corpus = create_corpus(fake_df[text_field].values)\n",
        "fake_top_n_words, fake_symptom_dict = top_words(fake_corpus, top_n=top_n, return_dict=True)\n",
        "fake_words, fake_word_counts = zip(*fake_top_n_words)\n",
        "\n",
        "def plot_words(word_list, word_counts, n, text_description, figsize=(15,5)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.bar(word_list, word_counts)\n",
        "    plt.title(f\"Top {n} words in {text_description}\", weight='bold')\n",
        "    plt.ylabel(\"Word Count\", weight='bold')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SousdCSHQOpJ"
      },
      "source": [
        "plot_words(fake_words, fake_word_counts, 50, \"Fake Article Titles\")\n",
        "print(f\"Total unique words in {text_field}: {len(fake_symptom_dict)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UpAXV7RQ-AY"
      },
      "source": [
        "top_n = 50\n",
        "text_field = \"search\"\n",
        "\n",
        "true_corpus = create_corpus(true_df[text_field].values)\n",
        "true_top_n_words, true_symptom_dict = top_words(true_corpus, top_n=top_n, return_dict=True)\n",
        "true_words, true_word_counts = zip(*true_top_n_words)\n",
        "\n",
        "plot_words(true_words, true_word_counts, 50, \"True Article Titles\")\n",
        "print(f\"Total unique words in {text_field}: {len(true_symptom_dict)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1LQn4W1R-oK"
      },
      "source": [
        "Cleaning and pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to-wn0WoSAJC"
      },
      "source": [
        "def clean_and_tokenise(tweet, stop_words=False, stem=False, lemmatize=False):\n",
        "    \"\"\" Text cleaning function - lowercase and remove stopwords \"\"\"\n",
        "    cleaned_text = re.sub('<[^>]*>', '', tweet.lower())\n",
        "    \n",
        "    # remove custom unwanted characters from our text\n",
        "    cleaned_text = remove_badchars(cleaned_text)\n",
        "    \n",
        "    # apply stop-word, stemming/lemmatising as required\n",
        "    if stop_words:\n",
        "        tokens = [word for word in tokenise(cleaned_text, stem=stem, \n",
        "                                            lemmatize=lemmatize) if word not in sw]\n",
        "    else:\n",
        "        tokens = [word for word in tokenise(cleaned_text, stem=stem, lemmatize=lemmatize)]\n",
        "    \n",
        "    cleaned_text = \" \".join(tokens)\n",
        "    \n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def remove_badchars(tweet):\n",
        "    \"\"\" Remove certain unwanted symbols and chars \"\"\"\n",
        "    delete_chars = \"[]()@''+&'\"\n",
        "    space_chars = \"_.-\"\n",
        "    table = dict((ord(c), \" \") for c in space_chars)\n",
        "    table.update(dict((ord(c), None) for c in delete_chars))\n",
        "    return tweet.translate(table)\n",
        "\n",
        "\n",
        "def tokenise(tweet, stem=False, lemmatize=False):\n",
        "    \"\"\" Form tokenised stemmed text using a list comp and return \"\"\"\n",
        "    if lemmatize:\n",
        "        tokenised = [lemmatizer.lemmatize(word) for word in text.split()]\n",
        "    elif stem:\n",
        "        tokenised = [stemmer.stem(word) for word in text.split()]\n",
        "    else:\n",
        "        tokenised = [word for word in tweet.split()]\n",
        "    return tokenised"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9UjOdjSScYO"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "# stop words - append additionals if needed\n",
        "sw = stopwords.words('english')\n",
        "\n",
        "# clean text data - apply stopwords, but not lemmatisation / stemming this time\n",
        "data_df['cleaned_title'] = data_df['search'].apply(clean_and_tokenise, stop_words=False, \n",
        "                                                      stem=False, lemmatize=False)\n",
        "\n",
        "# clean text data - apply stopwords, but not lemmatisation / stemming this time\n",
        "data_df['cleaned_text'] = data_df['tweet'].apply(clean_and_tokenise, stop_words=False, \n",
        "                                                      stem=False, lemmatize=False)\n",
        "\n",
        "data_df['combined_text'] = data_df['cleaned_text'] + \" \" + data_df['cleaned_title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsrJhwScUVbR"
      },
      "source": [
        "true_df = data_df[data_df['fake'] == 0].copy()\n",
        "fake_df = data_df[data_df['fake'] == 1].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKy95vcGUjOv"
      },
      "source": [
        "X = data_df['combined_text']\n",
        "y = data_df['fake']\n",
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y8St3aMUnXg"
      },
      "source": [
        "nlp = spacy.load('en_core_web_lg', disable=[\"parser\", \"tagger\", \"ner\", \"textcat\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rMDcUJKWtOK"
      },
      "source": [
        "vectorised_features = np.array([nlp(x).vector for x in X])\n",
        "vectorised_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQcSq-kOYqQM"
      },
      "source": [
        "umap_embedder = umap.UMAP()\n",
        "%time umap_features = umap_embedder.fit_transform(vectorised_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgebhlXTZ_R2"
      },
      "source": [
        "# sns settings\n",
        "sns.set(rc={'figure.figsize':(12,8)})\n",
        "palette = sns.hls_palette(2, l=.4, s=.9)\n",
        "\n",
        "# plot UMAP projection with annotations from k-means clustering\n",
        "sns.scatterplot(umap_features[:,0], umap_features[:,1], \n",
        "                hue=y, legend='full', \n",
        "                palette=palette, s=50, alpha=0.1)\n",
        "\n",
        "plt.title(f\"UMAP Projection\", \n",
        "          weight='bold', size=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68Q6iiy1bEgb"
      },
      "source": [
        "def plot_LSA(word_vectors, word_labels, figsize=(12, 8), alpha=0.3):\n",
        "    \"\"\" Perform latent semantic analysis and plot results \"\"\"\n",
        "    lsa = TruncatedSVD(n_components=2)\n",
        "    lsa.fit(word_vectors)\n",
        "    lsa_scores = lsa.transform(word_vectors)\n",
        "    color_mapper = {label:idx for idx,label in enumerate(set(word_labels))}\n",
        "    color_column = [color_mapper[label] for label in word_labels]\n",
        "    colors = ['orange','blue']\n",
        "    \n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=alpha, \n",
        "                c=word_labels, cmap=ListedColormap(colors))\n",
        "    \n",
        "    orange_patch = mpatches.Patch(color='orange', label='Not')\n",
        "    blue_patch = mpatches.Patch(color='blue', label='Real')\n",
        "    plt.legend(handles=[orange_patch, blue_patch], prop={'size': 16})      \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAy-OAkObbF-"
      },
      "source": [
        "plot_LSA(vectorised_features, y, alpha=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEHD8y_0jPC6"
      },
      "source": [
        "def multi_model_cross_validation(clf_tuple_list, X, y, K_folds=10, score_type='accuracy', random_seed=0):\n",
        "    \"\"\" Find cross validation scores, and print and return results \"\"\"\n",
        "    \n",
        "    model_names, model_scores = [], []\n",
        "    \n",
        "    for name, model in clf_list:\n",
        "        k_fold = StratifiedKFold(n_splits=K_folds, shuffle=True, random_state=random_seed)\n",
        "        cross_val_results = cross_val_score(model, X, y, cv=k_fold, scoring=score_type, n_jobs=-1)\n",
        "        model_names.append(name)\n",
        "        model_scores.append(cross_val_results)\n",
        "        print(\"{0:<40} {1:.5f} +/- {2:.5f}\".format(name, cross_val_results.mean(), cross_val_results.std()))\n",
        "        \n",
        "    return model_names, model_scores\n",
        "\n",
        "\n",
        "def boxplot_comparison(model_names, model_scores, figsize=(12, 6), score_type=\"Accuracy\",\n",
        "                       title=\"Sentiment Analysis Classification Comparison\"):\n",
        "    \"\"\" Boxplot comparison of a range of models using Seaborn and matplotlib \"\"\"\n",
        "    \n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    fig.suptitle(title, fontsize=18)\n",
        "    ax = fig.add_subplot(111)\n",
        "    sns.boxplot(x=model_names, y=model_scores)\n",
        "    ax.set_xticklabels(model_names)\n",
        "    ax.set_xlabel(\"Model\", fontsize=16) \n",
        "    ax.set_ylabel(\"Model Score ({})\".format(score_type), fontsize=16)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=60)\n",
        "    plt.show()\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpaQ1rpYVQse"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlzIbykDWJjR"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRZd7pNQTDbw"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}